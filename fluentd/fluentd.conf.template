#
# docker logs
#

<source>
  type tail
  format json
  read_from_head true
  time_key time
  path /var/lib/docker/containers/*/*-json.log
  pos_file /var/lib/docker/containers/containers.log.pos
  time_format %Y-%m-%dT%H:%M:%S
  tag docker.*
</source>

<match docker.var.lib.docker.containers.*.*.log>
  type record_reformer
  container_id ${tag_parts[5]}
  hostname @NODENAME@
  tag cleanup.docker
</match>

<match docker>
  type elasticsearch
  host @ES_HOST@
  logstash_format true
  logstash_prefix docker
  flush_interval 10s
</match>

#
# rsyslog logs
#

<source>
  type syslog
  format /^(?<time>[^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?[^\:]*\: *(?<message>.*)$/
  time_format %Y-%m-%dT%H:%M:%S.%L%:z
  tag cleanup.syslog
</source>

# Workaround to make search by exact host names. Since elasticsearch 2.x there
# is no support for startup configs index mapping, so we cannot specify which
# fields must not be analyzed. So we add for host corresponding field with
# md5 hex digest of host value.
# When a search will be performed there must be the same conversion for the
# 'host_md5' term.
# TODO: If there will be support in fluentd ES plugin to index templates like
# in logstash, then replace it with some index template.
<filter cleanup.**>
  type digestadd
  md5 {"host_md5": "host", "hostname_md5": "hostname"}
</filter>


<match syslog.**>
  type elasticsearch
  host @ES_HOST@
  logstash_format true
  logstash_prefix syslog
  flush_interval 10s
</match>

#
# nanoseconds
#

<match cleanup.**>
  type record_reformer
  time_nano ${t = Time.now; ((t.to_i * 1000000000) + t.nsec).to_s}
  tag ${tag_suffix[1]}
</match>
